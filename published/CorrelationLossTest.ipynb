{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CorrelationLossTest","version":"0.3.2","provenance":[{"file_id":"1zC8GTH2kM0FuPsVGcEedhv9i0O_7e3Ba","timestamp":1563135113835},{"file_id":"1GjgjiG19p0gePOZs788-mvcJGb4QYKbp","timestamp":1562872240851},{"file_id":"1YRbc6F_EdBO40uCFhrpbkwEhHOVK5Eh8","timestamp":1562568323078},{"file_id":"1BuNdux1pzaNikRat2VEVOpRvnxLe1Exp","timestamp":1561377030766}],"private_outputs":true,"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"jLnr6JerraGn","colab_type":"text"},"source":["# Use pearson correlation coefficient (for linear regression) as a loss function\n","\n","A wild try: when we try to predict a value where random noise is involved, it might be a good idea to use the regression coefficient for a linear regression between our prediction and the real value as a loss function and metric. In the ideal case, the prediction would be equal to the real value, we'd have a regression coefficient 1.\n","\n","Of course, the regression coefficient leaves too much freedom for the prediction: the prediction can be any linear function of the real value. So we have to introduce an additional loss, such that these are close. There are several ways: we could add the squared difference of the means and variations of both distributions multiplied with some low factor, to make sure they are equal. Or we could add the \"conventional\" mean squared difference multiplied with a low factor. You name it.\n","\n","\n","I'm not sure how much sense that makes, but let's try. It might work well if we try to predict something that is not far from linear but with lots of noise."]},{"cell_type":"code","metadata":{"id":"RVDe2x-XrOuq","colab_type":"code","colab":{}},"source":["#@title Imports and utilities {display-mode: \"form\"}\n","\n","\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.python.keras import backend as K\n","from tensorflow.python.ops import array_ops\n","from tensorflow.python.ops import math_ops\n","\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import sklearn as sk\n","import numpy as np\n","import functools\n","import itertools\n","import time\n","import os\n","import datetime\n","import re\n","import altair as alt\n","\n","print([tf.__version__, pd.__version__, sns.__version__ ,plt.matplotlib.__version__,np.__version__])\n","\n","from IPython.core.pylabtools import figsize\n","figsize(12.5, 3)\n","\n","# Display training progress by printing a single dot for each completed epoch\n","class PrintDot(tf.keras.callbacks.Callback):\n","  def on_epoch_end(self, epoch, logs):\n","    if epoch % 100 == 0: print('')\n","    print('.', end='')\n","\n","early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, min_delta=0.001, verbose=1, restore_best_weights=True)\n","terminate_nan = keras.callbacks.TerminateOnNaN()\n","\n","def plot_history(history):\n","    hist = pd.DataFrame(history.history)\n","    hist['epoch'] = history.epoch\n","\n","    plt.figure()\n","    plt.xlabel('Epoch')\n","    plt.ylabel('Val Loss')\n","    plt.semilogy(hist['epoch'], hist['loss'], label='Train Error')\n","    plt.plot(hist['epoch'], hist['val_loss'], label='Val Error')\n","    # plt.ylim([0,5])\n","    plt.legend()\n","    \n","def meanAndVariance(y_true, y_pred):\n","  y_pred = tf.convert_to_tensor(y_pred)\n","  y_true = math_ops.cast(y_true, y_pred.dtype)\n","  means = y_pred[..., 0::2]\n","  variances = y_pred[..., 1::2]\n","  res = K.square(means - y_true) + K.square(variances - K.square(means - y_true))\n","  return K.mean(res, axis=-1)\n","\n","def nans(ds):\n","  return ds.loc[ds.isna().any(axis=1)]\n","\n","def plothisto(df, col=None):\n","  \"\"\"Plots an interactive histogram of a (default first) column of a dataset\"\"\"\n","  col = col if (col) else df.columns[0]\n","  return alt.Chart(df.sample(5000).reset_index()).mark_bar().encode(\n","    x=alt.X(col, bin= alt.BinParams(maxbins=200) ),\n","    y='count()',\n","  ).interactive(bind_y=False)\n","\n","%config IPCompleter.greedy=True"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfdGQyAJJvC2","colab_type":"code","colab":{}},"source":["tf.enable_eager_execution()\n","#%xmode Verbose\n","#%xmode Context\n","\n","def t(a):\n","  \"\"\"For testing: generate a float64 tensor from anything.\"\"\"\n","  return tf.constant(a, dtype=tf.float64)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"AbbRCl5jiTpQ","colab_type":"code","colab":{}},"source":["def tmean(x, axis=-1):\n","  \"\"\"Arithmetic mean of a tensor over some axis, default last.\"\"\"\n","  x = tf.convert_to_tensor(x)\n","  sum = tf.reduce_sum(x, axis=axis)\n","  n = tf.cast(tf.shape(x)[axis], x.dtype)\n","  return sum / n\n","\n","tmean(t([[1.0],[2.0],[3.0]]), axis=-2)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FeZtNWH1wbEI","colab_type":"text"},"source":["Pearson correlation coefficlent : $r_{xy} = \\frac{\\sum\\left((x-\\overline{x})(y-\\overline{y})\\right)}{\\sqrt{\\sum(x-\\overline{x})^2\\sum(y-\\overline{y})^2}} $"]},{"cell_type":"code","metadata":{"id":"bsm51OEKJ6KL","colab_type":"code","colab":{}},"source":["def correlationMetric(x, y, axis=-2):\n","  \"\"\"Metric returning the Pearson correlation coefficient of two tensors over some axis, default -2.\"\"\"\n","  x = tf.convert_to_tensor(x)\n","  y = math_ops.cast(y, x.dtype)\n","  n = tf.cast(tf.shape(x)[axis], x.dtype)\n","  xsum = tf.reduce_sum(x, axis=axis)\n","  ysum = tf.reduce_sum(y, axis=axis)\n","  xmean = xsum / n\n","  ymean = ysum / n\n","  xvar = tf.reduce_sum( tf.squared_difference(x, xmean), axis=axis)\n","  yvar = tf.reduce_sum( tf.squared_difference(y, ymean), axis=axis)\n","  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n","  corr = cov / tf.sqrt(xvar * yvar)\n","  return tf.constant(1.0, dtype=x.dtype) - corr\n","\n","correlationMetric(tf.constant([[0.0, 1.0, 2.0]]), tf.constant([[1.0, 3.0, 2.0]]), axis=-1)\n","correlationMetric(tf.constant([[0.0, 2.0, 1.0]]), tf.constant([[1.0, 3.0, 2.0]]), axis=-1)\n","correlationMetric(tf.constant([[0.0], [2.0], [1.0]]), tf.constant([[1.0], [3.0], [2.0]]), axis=-2)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qQ6MlxYEOU7I","colab_type":"code","colab":{}},"source":["def correlationLoss(x,y, axis=-2):\n","  \"\"\"Loss function that maximizes the pearson correlation coefficient between the predicted values and the labels,\n","  while trying to have the same mean and variance\"\"\"\n","  x = tf.convert_to_tensor(x)\n","  y = math_ops.cast(y, x.dtype)\n","  n = tf.cast(tf.shape(x)[axis], x.dtype)\n","  xsum = tf.reduce_sum(x, axis=axis)\n","  ysum = tf.reduce_sum(y, axis=axis)\n","  xmean = xsum / n\n","  ymean = ysum / n\n","  xsqsum = tf.reduce_sum( tf.squared_difference(x, xmean), axis=axis)\n","  ysqsum = tf.reduce_sum( tf.squared_difference(y, ymean), axis=axis)\n","  cov = tf.reduce_sum( (x - xmean) * (y - ymean), axis=axis)\n","  corr = cov / tf.sqrt(xsqsum * ysqsum)\n","  # absdif = tmean(tf.abs(x - y), axis=axis) / tf.sqrt(yvar)\n","  sqdif = tf.reduce_sum(tf.squared_difference(x, y), axis=axis) / n / tf.sqrt(ysqsum / n)\n","  # meandif = tf.abs(xmean - ymean) / tf.abs(ymean)\n","  # vardif = tf.abs(xvar - yvar) / yvar\n","  # return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (meandif * 0.01) + (vardif * 0.01)) , dtype=tf.float32 )\n","  return tf.convert_to_tensor( K.mean(tf.constant(1.0, dtype=x.dtype) - corr + (0.01 * sqdif)) , dtype=tf.float32 )\n","\n","#t1,t2 = tf.constant([[0.0], [3.0], [2.0]]), tf.constant([[0.0], [1.0], [3.0]])\n","#print(correlationLoss(t1,t2))\n","#print(correlationMetric(t1,t2))\n","#del t1,t2"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"s5ba8x6g46sr","colab_type":"code","colab":{}},"source":["samples = 10240\n","epochs = 100\n","steps = 1024\n","batchsize = 128\n","validationsamples = samples // 10"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kMr7Osc1UF_J","colab_type":"text"},"source":["As testdata we use a simple example with gaussian random inputs x_d and r_d ( \\_d is a postfix that marks the column as data to learn from), some nonlinear function to approximate and add a little noise dependent on the inputs. The postfix \\_g designates the column as \"goal\" to approximate."]},{"cell_type":"code","metadata":{"id":"egi-8Tq5jNVf","colab_type":"code","colab":{}},"source":["data = pd.DataFrame( np.random.randn(samples,2), columns=['x_d', 'r_d'] ).sample(frac=1.0)\n","data['y_g'] = data.x_d * data.x_d + 1\n","data.y_g = data.y_g + np.random.randn(samples) * ( np.cos(3*data.x_d) ) * 0.5\n","data['z_g'] = data.x_d * data.r_d + 3\n","data.z_g = data.z_g + np.random.randn(samples) * data.x_d * data.r_d * 0.1\n","\n","train_data = data.filter(regex='_d').to_numpy()\n","train_labels = data.filter(regex='_g').to_numpy()\n","print(train_data.shape, train_labels.shape)\n","data.sample(3)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"-gkz4ta7u7Ag","colab_type":"code","colab":{}},"source":["traindataset = tf.data.Dataset.from_tensor_slices((train_data, train_labels))\n","traindataset = traindataset.shuffle(samples)\n","\n","validationdataset = traindataset.take(validationsamples)\n","traindataset = traindataset.skip(validationsamples)\n","\n","traindataset = traindataset.repeat().batch(batchsize, drop_remainder=True)\n","validationdataset = validationdataset.repeat().batch(batchsize, drop_remainder=True)\n","# traindataset.make_one_shot_iterator().get_next()\n","# validationdataset.make_one_shot_iterator().get_next()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"kU7Nr-HCZSmN","colab_type":"code","colab":{}},"source":["# loss = tf.losses.mean_squared_error\n","loss = correlationLoss\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Dense(64, activation=tf.nn.softsign))\n","model.add(tf.keras.layers.Dense(32, activation=tf.nn.softsign))\n","model.add(tf.keras.layers.Dense(2))\n","# just a linear transformation that makes it easy to scale / shift the prediction right\n","# model.add(tf.keras.layers.Dense(2))\n","model.compile(loss=loss , optimizer='adam', metrics=[loss, correlationMetric, \"mse\", \"mae\"])\n","\n","model.fit(traindataset, steps_per_epoch=1, epochs=1) # this initializes the input_shape\n","model.summary()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"tISLFzX3MkPL","colab_type":"code","colab":{}},"source":["%time history = model.fit(traindataset, epochs=epochs, steps_per_epoch=steps, validation_data=validationdataset, validation_steps=validationsamples // batchsize, callbacks=[terminate_nan, early_stop])\n","#%time history = model.fit(tdataset, epochs=epochs, steps_per_epoch=steps, validation_split=0.1, callbacks=[terminate_nan])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"wknfioqHMw8C","colab_type":"code","colab":{}},"source":["print(history.history['loss'][-1], np.sqrt(history.history['loss'][-1]))\n","print(history.history['val_loss'][-1], np.sqrt(history.history['val_loss'][-1]))\n","plot_history(history)\n","# print(datetime.datetime.now())\n","pd.DataFrame(history.history).plot(logy=True, figsize=(12.5,12.5))\n","model.summary()\n","history.params"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ozOAhBI45JL9","colab_type":"code","colab":{}},"source":["p = pd.DataFrame(model.predict(data.filter(regex='_d').to_numpy()))\n","p.columns = data.filter(regex='_g').columns.str.replace('_g','_p')\n","p.index = data.index\n","data = data.join(p)\n","data['y_o'] = data.y_p - data.y_g  # \"offset\"\n","data['z_o'] = data.z_p - data.z_g \n","data.sample(3)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fBSy5Sfcu33E","colab_type":"text"},"source":["# Conclusion\n","\n","It seems to work, indeed, though it converges in our example more slowly than, for example, mean squared errors. The next diagram shows the prediction, which looks very much like the real value below. The third diagram shows the difference.\n","\n","I'm not sure how much it's worth, but this might be another tool in our toolbox if there is randomness involved."]},{"cell_type":"code","metadata":{"id":"rdZgkXAXFwZR","colab_type":"code","colab":{}},"source":["# let's see how we are doing on the correlation\n","data.corr()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qEgcL1355eID","colab_type":"code","colab":{}},"source":["alt.Chart(data.sample(1000).reset_index()).mark_point(size=0.1).encode(\n","  x='x_d',\n","  y='y_p'\n",").interactive()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"qHTLwQP9AiYj","colab_type":"code","colab":{}},"source":["alt.Chart(data.sample(1000).reset_index()).mark_point(size=0.1).encode(\n","  x='x_d',\n","  y='y_g'\n",").interactive()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"2SP-BxMKt_xp","colab_type":"code","colab":{}},"source":["alt.Chart(data.sample(1000).reset_index()).mark_point(size=0.1).encode(\n","  x='x_d',\n","  y='y_o'\n",").interactive()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"CwN2o5H580Yd","colab_type":"code","colab":{}},"source":["# Interesting is also how the difference depends on p - which is what we correlate\n","\n","alt.Chart(data.sample(1000).reset_index()).mark_point(size=0.1).encode(\n","  x='y_p',\n","  y='y_o'\n",").interactive()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZXhL8d8PGRkt","colab_type":"code","colab":{}},"source":["# that's the actual data on which we maximize the correlation\n","\n","alt.Chart(data.sample(1000).reset_index()).mark_point(size=0.1).encode(\n","  x='y_p',\n","  y='y_g'\n",").interactive()"],"execution_count":0,"outputs":[]}]}